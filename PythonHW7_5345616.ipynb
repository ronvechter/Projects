{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Homework 7\n",
    "\n",
    "**Release date:** Saturday, November 23 <br>\n",
    "**Due date:** Friday, __December 6__, 11:59 p.m. via GauchoSpace\n",
    "\n",
    "**Instruction:** Please upload your jupyter notebook on GauchoSpace with filename \"PythonHW7_YOURPERMNUMBER.ipynb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Background:__ Markov chains could be used to model a plethora of phenomena that happen in our world. The only assumption that we would have to accept is the fact that what we are trying to model depends only on the last step, and not on all previous steps (the whole history). \n",
    "\n",
    "For example, Sahin and Sen (2001) model hourly wind speeds in a NW part of Turkey as a Markov chain ${(X_n)}_{n\\in \\mathbb{N}}$ with 7 states representing different wind speed levels. Since in Python arrays are indexed starting from $0$, let us consider the states to be $S=\\{0,1,2,3,4,5,6 \\}$, with $0$ representing the lowest wind speed level. The transition matrix is given by: \n",
    "\n",
    "\\begin{gather*}\n",
    "P=\\begin{array}{cccccccc}\n",
    "& 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n",
    "0 & 0.756 & 0.113 & 0.129 & 0.002 & 0 & 0 & 0\\\\\n",
    "1 & 0.174 & 0.821 & 0.004 & 0.001 & 0 & 0 & 0\\\\\n",
    "2 & 0.141 & 0.001 & 0.776 & 0.082 & 0 & 0 & 0\\\\\n",
    "3 & 0.003 & 0 & 0.192 & 0.753 & 0.052 & 0 & 0\\\\\n",
    "4 & 0 & 0 & 0.002 & 0.227 & 0.735 & 0.036 & 0\\\\\n",
    "5 & 0 & 0 & 0 & 0.007 & 0.367 & 0.604 & 0.022\\\\\n",
    "6 & 0 & 0 & 0 & 0 & 0.053 & 0.158 & 0.789\\\\\n",
    "\\end{array}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we start with loading some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import linalg \n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (4 Points)\n",
    "\n",
    "1. Implement the transition probability matrix $P$ from above as a two dimensional <tt>numpy.array()</tt>. Compute $P^{250}$ and list at least two of its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04],\n",
       "       [3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04],\n",
       "       [3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04],\n",
       "       [3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04],\n",
       "       [3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04],\n",
       "       [3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04],\n",
       "       [3.24586174e-01, 2.06604292e-01, 3.03930586e-01, 1.31889029e-01,\n",
       "        2.98620155e-02, 2.83256580e-03, 2.95338614e-04]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE! \n",
    "transMatrix = np.array([[0.756, 0.113, 0.129, 0.002, 0, 0, 0],\n",
    "                        [0.174, 0.821, 0.004, 0.001, 0, 0, 0],\n",
    "                        [0.141, 0.001, 0.776, 0.082, 0, 0, 0],\n",
    "                        [0.003, 0, 0.192, 0.753, 0.052, 0, 0],\n",
    "                        [0, 0, 0.002, 0.227, 0.735, 0.036, 0],\n",
    "                        [0, 0, 0, 0.007, 0.367, 0.604, 0.022],\n",
    "                        [0, 0, 0, 0, 0.053, 0.158, 0.789]])\n",
    "trans250 = np.linalg.matrix_power(transMatrix, 250)\n",
    "trans250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the stationary distribution $\\pi$ as a right-eigenvector of $P^T$ to the eigenvalue 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37796447, -0.02093918, -0.10674212, -0.0630621 , -0.05869223,\n",
       "       -0.00518262,  0.00043537])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE! \n",
    "## HINT: USE scipy.linalg.eig(..) AND CHECK THE DOCUMENTATION\n",
    "eigenval, eigenvec = scipy.linalg.eig(transMatrix)\n",
    "stationary = eigenvec[0]\n",
    "stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (6 Points)\n",
    "\n",
    "Recall from class that the following theorem holds true:\n",
    "\n",
    "<b>Theorem:</b> For any finite irreducible Markov chain we have that the stationary distribution $\\pi$ satisfies\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pi_j=\\frac{1}{\\mathbb{E}[T_j\\,| \\,X_0=j]} \\quad \\text{ for all } j \\in \\mathcal{S} \n",
    "\\end{equation*}\n",
    "\n",
    "where $T_j = \\min\\{n>0:X_n=j \\}$ denotes the first visiting time of state $j$ after having started in $j$ at time 0.\n",
    "\n",
    "Hence, in order to find the expected return time to state $j$, we just have to compute $1/\\pi_j$.\n",
    "\n",
    "Check numerically that this theorem holds for state $0$. That is, simulate $N=10^5$ realizations of the Markov Chain with transition matrix $P$ that start in state $0$. Each Markov chain should be simulated until state $0$ is reached again (so you do not actually have to run each Markov chain for many steps). For each of the $N$ simulations, memorize how many steps it took to get back to state $0$. A good estimate of $\\mathbb{E}[T_0 \\,| \\, X_0=0]$ will then be the average of all of those times. Because of the above theorem, the estimate should be close to $1/\\pi_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.10155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR OWN CODE HERE! \n",
    "n = 10**5\n",
    "i = 0\n",
    "times = np.zeros(n)\n",
    "while i<n:\n",
    "    state = np.random.choice([0,1,2,3,4,5,6], p = transMatrix[0])\n",
    "    count = 1\n",
    "    while state != 0:\n",
    "        count += 1\n",
    "        state = np.random.choice([0,1,2,3,4,5,6], p = transMatrix[state])\n",
    "    times[i] = count\n",
    "    i += 1\n",
    "np.mean(times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
